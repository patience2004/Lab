
lab 1 ==
# Import necessary libraries
import pandas as pd
import numpy as np

# 1. Combining and Merging Datasets

# Create two sample DataFrames
sales_data_1 = pd.DataFrame({
    'OrderID': [1, 2, 3, 4],
    'Product': ['Laptop', 'Tablet', 'Smartphone', 'Headphones'],
    'Sales': [1200, 800, 1500, 300]
})

sales_data_2 = pd.DataFrame({
    'OrderID': [3, 4, 5, 6],
    'Product': ['Smartphone', 'Headphones', 'Smartwatch', 'Tablet'],
    'Sales': [1500, 300, 200, 900]
})

# Display the DataFrames
print("Sales Data 1:\n", sales_data_1)
print("\nSales Data 2:\n", sales_data_2)

# Merge DataFrames based on 'OrderID' using an inner join
merged_data = pd.merge(sales_data_1, sales_data_2, on='OrderID', how='inner', suffixes=('_left', '_right'))
print("\nMerged Data (Inner Join):\n", merged_data)

# Concatenate the DataFrames vertically
combined_data = pd.concat([sales_data_1, sales_data_2], ignore_index=True)
print("\nCombined Data (Concatenated Vertically):\n", combined_data)

# 2. Reshaping Data with Melt

# Create a sample DataFrame for reshaping
reshaping_data = pd.DataFrame({
    'Month': ['Jan', 'Feb', 'Mar'],
    'Product_A': [100, 150, 130],
    'Product_B': [90, 80, 120]
})

print("\nReshaping Data (Original):\n", reshaping_data)

# Melt the DataFrame to reshape it from wide to long format
melted_data = pd.melt(reshaping_data, id_vars=['Month'], var_name='Product', value_name='Sales')
print("\nMelted Data (Long Format):\n", melted_data)

# 3. Pivoting Data

# Create a sample DataFrame for pivoting
pivot_data = pd.DataFrame({
    'Month': ['Jan', 'Jan', 'Feb', 'Feb', 'Mar', 'Mar'],
    'Product': ['Product_A', 'Product_B', 'Product_A', 'Product_B', 'Product_A', 'Product_B'],
    'Sales': [100, 90, 150, 80, 130, 120]
})

print("\nPivot Data (Original):\n", pivot_data)

# Pivot the DataFrame to reshape it back to wide format
pivoted_data = pivot_data.pivot(index='Month', columns='Product', values='Sales')
print("\nPivoted Data (Wide Format):\n", pivoted_data)

# 4. Handling Missing Data

# Introduce some missing values
pivoted_data.loc['Feb', 'Product_A'] = np.nan
pivoted_data.loc['Mar', 'Product_B'] = np.nan
print("\nPivoted Data with Missing Values:\n", pivoted_data)

# Fill missing values with the mean of each column
filled_data = pivoted_data.fillna(pivoted_data.mean())
print("\nFilled Data (Missing Values Handled):\n", filled_data)

# 5. Summary Statistics
print("\nSummary Statistics of Filled Data:\n", filled_data.describe())


Sample output:
Sales Data 1:
    OrderID      Product  Sales
0        1       Laptop   1200
1        2       Tablet    800
2        3   Smartphone   1500
3        4   Headphones    300



lab == 2

String Manupulation:
# Sample text to work with
text = "  Hello, World! Welcome to Python programming.  "

# 1. Strip leading and trailing spaces
clean_text = text.strip()
print(f"Original Text: '{text}'")
print(f"Text after stripping spaces: '{clean_text}'")

# 2. Convert the text to uppercase
upper_text = clean_text.upper()
print(f"\nText in uppercase: '{upper_text}'")

# 3. Convert the text to lowercase
lower_text = clean_text.lower()
print(f"\nText in lowercase: '{lower_text}'")

# 4. Count occurrences of a substring (e.g., "o")
count_o = clean_text.count("o")
print(f"\nNumber of occurrences of 'o': {count_o}")

# 5. Replace a word in the string
replaced_text = clean_text.replace("Python", "Data Science")
print(f"\nText after replacing 'Python' with 'Data Science': '{replaced_text}'")

# 6. Find the position of a word in the string
position_world = clean_text.find("World")
print(f"\nPosition of 'World' in the text: {position_world}")

# 7. Split the text into words (by default on spaces)
words = clean_text.split()
print(f"\nList of words in the text: {words}")

# 8. Join the words back into a single string
joined_text = " ".join(words)
print(f"\nText after joining words: '{joined_text}'")

# 9. Check if the text starts with "Hello"
starts_with_hello = clean_text.startswith("Hello")
print(f"\nDoes the text start with 'Hello'? {starts_with_hello}")

# 10. Check if the text ends with a specific word (e.g., "programming.")
ends_with_programming = clean_text.endswith("programming.")
print(f"\nDoes the text end with 'programming.'? {ends_with_programming}")

output:
Original Text: '  Hello, World! Welcome to Python programming.  '
Text after stripping spaces: 'Hello, World! Welcome to Python programming.'

Text in uppercase: 'HELLO, WORLD! WELCOME TO PYTHON PROGRAMMING.'

Text in lowercase: 'hello, world! welcome to python programming.'


lab 2 === b

Regular Expressions:

import re

# Sample text
text = """
John's email is [email protected]. He said, "Python is awesome!!"    It's a     great   language.
Another email: [email protected]. 
"""

# 1. Remove special characters except for spaces and email-related characters.
# Using regex to remove non-alphabetic characters and non-email symbols
clean_text = re.sub(r"[^a-zA-Z0-9@\.\s]", "", text)
print("Text after removing special characters:")
print(clean_text)

# 2. Convert the text to lowercase
clean_text = clean_text.lower()
print("\nText after converting to lowercase:")
print(clean_text)

# 3. Replace multiple spaces with a single space
clean_text = re.sub(r"\s+", " ", clean_text)
print("\nText after replacing multiple spaces:")
print(clean_text)

# 4. Extract all words starting with a vowel (a, e, i, o, u)
vowel_words = re.findall(r"\b[aeiouAEIOU]\w+", clean_text)
print("\nWords starting with a vowel:")
print(vowel_words)

# 5. Replace email addresses with '[email protected]'
masked_text = re.sub(r"\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b", "[email protected]", clean_text)
print("\nText after replacing emails:")
print(masked_text)

output ==
Text after removing special characters:
johns email is john.doe@example.com. he said python is awesome its a great language another email jane@example.com

Text after converting to lowercase:
johns email is john.doe@example.com. he said python is awesome its a great language another email jane@example.com

Text after replacing multiple spaces:
johns email is john.doe@example.com. he said python is awesome its a great language another email jane@example.com

Words starting with a vowel:
['email', 'is', 'awesome', 'another', 'email']

Text after replacing emails:
johns email is [email protected]. he said python is awesome its a great language another email [email protected]


lab 3 ==
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from statsmodels.tsa.holtwinters import ExponentialSmoothing

# Create sample time series data
np.random.seed(42)
date_range = pd.date_range(start="2022-01-01", end="2023-01-01", freq="D")
data = pd.DataFrame({
    "Date": date_range,
    "Value_A": np.random.normal(100, 10, len(date_range)),
    "Value_B": np.random.normal(200, 20, len(date_range)),
})

# Set Date as the index
data.set_index("Date", inplace=True)

# GroupBy Mechanics
def groupby_mechanics(data):
    print("\n--- GroupBy Mechanics ---")
    # Group data by month and calculate mean
    grouped = data.resample('M').mean()
    print(grouped)
    return grouped

# Data Formats: Vector and Multivariate
def data_formats(data):
    print("\n--- Data Formats ---")
    # Display data as vector
    print("\nVector Format:")
    print(data["Value_A"].head())

    # Display multivariate time series
    print("\nMultivariate Time Series:")
    print(data.head())

# Forecasting Example
def time_series_forecasting(data):
    print("\n--- Forecasting ---")
    # Select a single column for forecasting
    ts = data["Value_A"]

    # Train-Test Split
    train = ts[:int(0.8 * len(ts))]
    test = ts[int(0.8 * len(ts)):]

    # Fit the Holt-Winters Exponential Smoothing model
    model = ExponentialSmoothing(train, seasonal="add", seasonal_periods=30).fit()

    # Forecast for the test period
    forecast = model.forecast(len(test))

    # Plot results
    plt.figure(figsize=(12, 6))
    plt.plot(train, label="Train")
    plt.plot(test, label="Test")
    plt.plot(forecast, label="Forecast")
    plt.legend()
    plt.title("Time Series Forecasting")
    plt.show()

# Main function
if __name__ == "__main__":
    print("--- Time Series Data ---")
    print(data.head())

    # Grouping Mechanics
    monthly_data = groupby_mechanics(data)

    # Data Formats
    data_formats(data)

    # Time Series Forecasting
    time_series_forecasting(data)



Sample output:
--- Time Series Data ---
               Value_A     Value_B
Date                              
2022-01-01  104.967142  204.481850
2022-01-02   98.617357  200.251848
2022-01-03  106.476885  201.953522
2022-01-04  115.230299  184.539804
2022-01-05   97.658466  200.490203

--- GroupBy Mechanics ---
               Value_A     Value_B
Date                              
2022-01-31   97.985125  202.137470
2022-02-28   98.568317  204.960833
2022-03-31  100.439383  194.956405
2022-04-30   99.797484  198.429574
2022-05-31   99.161855  199.020262


 lab 4===




# Import necessary libraries
import numpy as np
import pandas as pd

def calculate_statistics(data, frequencies):
    # Create a DataFrame for the frequency distribution
    df = pd.DataFrame({'Value': data, 'Frequency': frequencies})
    
    # Calculate the total number of observations
    total = df['Frequency'].sum()
    
    # Calculate mean
    df['Weighted_Value'] = df['Value'] * df['Frequency']
    mean = df['Weighted_Value'].sum() / total
    
    # Calculate median
    cumulative_frequency = df['Frequency'].cumsum()
    median_index = cumulative_frequency.searchsorted(total / 2)
    median = df['Value'][median_index]

    # Calculate mode
    mode = df['Value'][df['Frequency'].idxmax()]
    
    # Calculate variance and standard deviation
    variance = np.average((df['Value'] - mean) ** 2, weights=df['Frequency'])
    std_deviation = np.sqrt(variance)

    # Calculate mean deviation
    mean_deviation = np.average(np.abs(df['Value'] - mean), weights=df['Frequency'])
    
    # Calculate quartile deviation
    q1 = np.percentile(data, 25)
    q3 = np.percentile(data, 75)
    quartile_deviation = (q3 - q1) / 2

    return {
        'Mean': mean,
        'Median': median,
        'Mode': mode,
        'Variance': variance,
        'Standard Deviation': std_deviation,
        'Mean Deviation': mean_deviation,
        'Quartile Deviation': quartile_deviation
    }

# Get user input for data and frequencies
data_input = input("Enter the data values separated by commas (e.g., 10, 20, 30): ")
frequencies_input = input("Enter the corresponding frequencies separated by commas (e.g., 1, 2, 3): ")

# Convert input strings to lists of integers
data = list(map(int, data_input.split(',')))
frequencies = list(map(int, frequencies_input.split(',')))

# Calculate statistics
statistics = calculate_statistics(data, frequencies)

# Display the results
for stat, value in statistics.items():
    print(f"{stat}: {value:.2f}")



lab 5 ==

# Import necessary libraries
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split, KFold, LeaveOneOut
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.linear_model import LinearRegression
from sklearn.datasets import fetch_california_housing

# Load the California housing dataset
data = fetch_california_housing()
X = pd.DataFrame(data.data, columns=data.feature_names)
y = pd.Series(data.target)

# Function to calculate and display metrics
def display_metrics(y_true, y_pred):
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    mae = mean_absolute_error(y_true, y_pred)
    r2 = r2_score(y_true, y_pred)
    print(f"Root Mean Squared Error (RMSE): {rmse:.4f}")
    print(f"Mean Absolute Error (MAE): {mae:.4f}")
    print(f"R² Score: {r2:.4f}")
    return rmse, mae, r2

# Validation Set Approach
def validation_set_approach(X, y):
    print("Validation Set Approach:")
    # Split the dataset into training (80%) and validation (20%) sets
    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)
    
    # Initialize and train the model
    model = LinearRegression()
    model.fit(X_train, y_train)
    
    # Make predictions on the validation set
    y_pred = model.predict(X_val)
    
    # Display metrics
    display_metrics(y_val, y_pred)

# Leave-One-Out Cross-Validation (LOOCV) Approach
def loocv_approach(X, y):
    print("Leave-One-Out Cross-Validation (LOOCV):")
    loo = LeaveOneOut()
    y_true, y_pred = [], []
    
    # Loop through each sample using LOOCV
    for train_index, test_index in loo.split(X):
        X_train, X_test = X.iloc[train_index], X.iloc[test_index]
        y_train, y_test = y.iloc[train_index], y.iloc[test_index]
        
        # Initialize and train the model
        model = LinearRegression()
        model.fit(X_train, y_train)
        
        # Make prediction for the single test sample
        y_pred.append(model.predict(X_test)[0])
        y_true.append(y_test.iloc[0])
    
    # Display metrics
    display_metrics(y_true, y_pred)

# K-Fold Cross-Validation Approach
def kfold_approach(X, y, k=5):
    print(f"{k}-Fold Cross-Validation Approach:")
    kf = KFold(n_splits=k, shuffle=True, random_state=42)
    y_true, y_pred = [], []
    
    # Loop through each fold
    for train_index, test_index in kf.split(X):
        X_train, X_test = X.iloc[train_index], X.iloc[test_index]
        y_train, y_test = y.iloc[train_index], y.iloc[test_index]
        
        # Initialize and train the model
        model = LinearRegression()
        model.fit(X_train, y_train)
        
        # Make predictions on the test set
        y_pred.extend(model.predict(X_test))
        y_true.extend(y_test)
    
    # Display metrics
    display_metrics(y_true, y_pred)

# Main function to run all approaches
def main():
    print("Cross-Validation for RMSE, MAE, and R²:\n")
    validation_set_approach(X, y)
    print("\n")
    loocv_approach(X, y)
    print("\n")
    kfold_approach(X, y, k=5)  # You can change k for different K-Fold Cross-Validation

# Execute the main function
if __name__ == "__main__":
    main()



lab 6 ==

import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import norm, binom, poisson, bernoulli

def get_user_data():
    # Get the frequency distribution input from the user
    data_input = input("Enter the data values separated by commas (e.g., 10, 20, 30): ")
    frequencies_input = input("Enter the corresponding frequencies separated by commas (e.g., 2, 3, 4): ")
    
    # Convert the inputs into lists of integers
    data = list(map(int, data_input.split(',')))
    frequencies = list(map(int, frequencies_input.split(',')))
    
    return data, frequencies

def plot_normal_distribution(data, frequencies):
    # Fit and plot Normal distribution
    mean = np.mean(data)
    std_dev = np.std(data)
    
    x = np.linspace(min(data), max(data), 100)
    pdf = norm.pdf(x, mean, std_dev)

    plt.plot(x, pdf, 'r-', lw=2, label='Normal Distribution')
    plt.title('Normal Distribution')
    plt.xlabel('Value')
    plt.ylabel('Probability Density')
    plt.show()

def plot_binomial_distribution(data, frequencies):
    # Fit and plot Binomial distribution (assuming n is max(data) and p is mean/len(data))
    n = max(data)
    p = np.mean(data) / n
    
    x = np.arange(0, n+1)
    pmf = binom.pmf(x, n, p)

    plt.bar(x, pmf, alpha=0.7, color='b', label='Binomial Distribution')
    plt.title('Binomial Distribution')
    plt.xlabel('Value')
    plt.ylabel('Probability')
    plt.show()

def plot_poisson_distribution(data, frequencies):
    # Fit and plot Poisson distribution (lambda is the mean of the data)
    lam = np.mean(data)
    
    x = np.arange(0, max(data)+1)
    pmf = poisson.pmf(x, lam)

    plt.bar(x, pmf, alpha=0.7, color='g', label='Poisson Distribution')
    plt.title('Poisson Distribution')
    plt.xlabel('Value')
    plt.ylabel('Probability')
    plt.show()

def plot_bernoulli_distribution(data, frequencies):
    # Assuming binary outcome for Bernoulli
    success_prob = np.mean(data) / max(data)
    
    x = [0, 1]
    pmf = bernoulli.pmf(x, success_prob)

    plt.bar(x, pmf, alpha=0.7, color='purple', label='Bernoulli Distribution')
    plt.title('Bernoulli Distribution')
    plt.xlabel('Value')
    plt.ylabel('Probability')
    plt.show()

def analyze_distributions(data, frequencies):
    print("Analyzing Normal Distribution:")
    plot_normal_distribution(data, frequencies)
    
    print("Analyzing Binomial Distribution:")
    plot_binomial_distribution(data, frequencies)
    
    print("Analyzing Poisson Distribution:")
    plot_poisson_distribution(data, frequencies)
    
    print("Analyzing Bernoulli Distribution:")
    plot_bernoulli_distribution(data, frequencies)

# Main program
data, frequencies = get_user_data()
analyze_distributions(data, frequencies)








